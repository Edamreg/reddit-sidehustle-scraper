name: Scrape Reddit Top (Last 7 Days)

on:
  workflow_dispatch: {}
  schedule:
    - cron: "0 12 * * *"
    - cron: "50 12 * * FRI"

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install deps
        run: |
          python -V
          pip install -r requirements.txt

      - name: Run scraper
        env:
          REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}
          REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}
          USER_AGENT: ${{ secrets.USER_AGENT }}
          LIMIT_PER_SUB: "20"
          SLEEP_BETWEEN_CALLS: "1.6"
          COMMENTS_LIMIT: "60"
        run: |
          python scraper.py
          echo "List data dir after scrape:"
          ls -la data || true

      - name: Commit & push data
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/*.json
          git commit -m "Update scraped feed" || echo "No changes to commit"
          git push
                - name: Install Google Drive deps
        if: ${{ secrets.GCP_SA_KEY != '' && secrets.GDRIVE_FOLDER_ID != '' }}
        run: |
          pip install google-api-python-client>=2.137.0 google-auth>=2.33.0 google-auth-httplib2>=0.2.0

      - name: Upload latest.json and archive to Google Drive
        if: ${{ secrets.GCP_SA_KEY != '' && secrets.GDRIVE_FOLDER_ID != '' }}
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
          GDRIVE_FOLDER_ID: ${{ secrets.GDRIVE_FOLDER_ID }}
        run: |
          python -c "import os, json, pathlib, datetime as dt
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

DATA_DIR = pathlib.Path('data')
LATEST = DATA_DIR / 'latest.json'
assert LATEST.exists(), 'latest.json not found'

sa_info = json.loads(os.environ['GCP_SA_KEY'])
creds = Credentials.from_service_account_info(sa_info, scopes=['https://www.googleapis.com/auth/drive'])
drive = build('drive', 'v3', credentials=creds)
folder_id = os.environ['GDRIVE_FOLDER_ID']

try:
    scraped_at = json.loads(LATEST.read_text()).get('scraped_at_utc')
except Exception:
    scraped_at = None
date_str = (scraped_at or dt.datetime.utcnow().strftime('%Y-%m-%d')).split('T')[0]
archive_name = f'reddit_top_week_{date_str}.json'

def find_file(name):
    q = f\"name = '{name.replace(\"'\",\"\\'\")}' and '{folder_id}' in parents and trashed = false\"
    res = drive.files().list(q=q, fields='files(id,name)').execute()
    files = res.get('files', [])
    return files[0]['id'] if files else None

def upload_or_update(local, name):
    file_id = find_file(name)
    media = MediaFileUpload(str(local), mimetype='application/json', resumable=False)
    if file_id:
        drive.files().update(fileId=file_id, media_body=media).execute()
        print('Updated', name)
    else:
        drive.files().create(body={'name': name, 'parents': [folder_id]}, media_body=media, fields='id').execute()
        print('Created', name)

upload_or_update(LATEST, 'latest.json')
if not find_file(archive_name):
    media = MediaFileUpload(str(LATEST), mimetype='application/json', resumable=False)
    drive.files().create(body={'name': archive_name, 'parents': [folder_id]}, media_body=media, fields='id').execute()
    print('Archived', archive_name)
else:
    print('Archive exists, skipping')"

